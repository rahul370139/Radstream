# RadStream Medical Imaging Inference Container
# Based on NVIDIA Triton Inference Server for high-performance model serving

FROM nvcr.io/nvidia/tritonserver:23.10-py3

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV TRITON_MODEL_STORE=/models
ENV TRITON_SERVER_HTTP_PORT=8000
ENV TRITON_SERVER_GRPC_PORT=8001
ENV TRITON_SERVER_METRICS_PORT=8002

# Install additional Python packages
RUN pip install --no-cache-dir \
    boto3>=1.34.0 \
    numpy>=1.24.0 \
    pillow>=10.0.0 \
    torch>=2.0.0 \
    torchvision>=0.15.0 \
    scikit-learn>=1.3.0

# Create model directory structure
RUN mkdir -p /models/radstream_classifier/1
RUN mkdir -p /models/radstream_detector/1
RUN mkdir -p /models/radstream_encoder/1

# Copy model configuration files
COPY model_config.pbtxt /models/radstream_classifier/config.pbtxt
COPY model_config.pbtxt /models/radstream_detector/config.pbtxt
COPY model_config.pbtxt /models/radstream_encoder/config.pbtxt

# Copy model files (these would be downloaded from S3 or ECR in production)
# For now, we'll create placeholder model files
RUN echo "placeholder_model" > /models/radstream_classifier/1/model.pt
RUN echo "placeholder_model" > /models/radstream_detector/1/model.pt
RUN echo "placeholder_model" > /models/radstream_encoder/1/model.pt

# Copy custom Python backend (if needed)
COPY custom_backend/ /opt/tritonserver/backends/custom_radstream/

# Copy health check script
COPY health_check.py /opt/tritonserver/health_check.py
RUN chmod +x /opt/tritonserver/health_check.py

# Copy startup script
COPY start_triton.sh /opt/tritonserver/start_triton.sh
RUN chmod +x /opt/tritonserver/start_triton.sh

# Expose ports
EXPOSE 8000 8001 8002

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python /opt/tritonserver/health_check.py

# Set working directory
WORKDIR /opt/tritonserver

# Start Triton server
CMD ["/opt/tritonserver/start_triton.sh"]
